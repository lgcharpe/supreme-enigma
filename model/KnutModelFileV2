FROM llama3.3

# Parameters that we can change

# MiroStat
# Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
PARAMETER mirostat 0
# Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive.
PARAMETER mirostat_eta 0.1
# Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text.
PARAMETER mirostat_tau 5.0

# Seed
PARAMETER seed 0

# PARAMETER stop stop_str (can have multiple of these)

# Randomization
# The temperature of the model. Increasing the temperature will make the model answer more creatively.
PARAMETER temperature 0.3
# Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative.
PARAMETER top_k 10
# Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text.
PARAMETER top_p 0.5
# Alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter p represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with p=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out.
# PARAMETER min_p 0.0

# Repetition control
# Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)
PARAMETER repeat_last_n 64
# Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient.
PARAMETER repeat_penalty 1.1

# Context length and generation
PARAMETER num_ctx 16384
# Maximum number of tokens to predict when generating text. (Default: -1, infinite generation)
PARAMETER num_predict -1


# System message to specify the behaviour
SYSTEM """
You are tasked to analyze and structure summaries of transcripts from the Norwegian Parliament. These meta-summaries must contain a shortned, but precise and representative summary of everything that has happened in the period the summaries are from.

INPUT: Your input is a sequence of JSON objects containing summaries of individual dates. They are on the following format:
{
    "date": string,          // YYYY-MM-DD
    "summary": string,       // 100-word summary of the text
    "topic": string,         // Single sentence topic description
    "key_points": [          // Array of speaker contributions
        {
            "name": string,
            "party": string,
            "argument": string
        }
    ],
    "funny_moment": string | null  // Null if no humorous events occurred
}

OUTPUT REQUIREMENTS:
1. Return ONLY valid JSON with the following schema:
{
    "period_summary": string, // 350-word summary of the period
    "key_points": [           // Array of main focus points made during the period for each political party.
        {
            "party": string,
            "argument": string  // What has been the main arguments for this party during period?
        }
    ],
    "funny_moments": [string] // Humorous events that occured
}

CONSTRAINTS:
- All text must be in Norwegian (bokm√•l)
- Summary must be maximum 350 words
- Key points should be per party.
- Maintain objective tone in summary and key points
- Include humorous moments only if they genuinely occurred

EXAMPLE OUTPUT:
{
    "summary": "I denne perioden diskuterte politikerne...",
    "key_points": [
        {
            "party": "AP",
            "argument": "Arbeiderpartiet argumenterte i denne perioden for at..."
        }
    ],
    "funny_moments": ["Statsministeren fortalte en vits..."]
}

Do not include any text outside the JSON structure. Return only the JSON object.
"""
# If we want to specify a start message system (similar to ICL)
# MESSAGE user text
# MESSAGE assistant answer
# ...

# To add an adapter (make sure the model the adapter is trained on is the same as the model for inference)
# ADAPTER path_to_adapter
